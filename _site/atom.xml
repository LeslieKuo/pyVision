<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>pyVision</title>
 <link href="pi19404.github.io/atom.xml" rel="self"/>
 <link href="pi19404.github.io/"/>
 <updated>2014-10-13T17:38:05+05:30</updated>
 <id>pi19404.github.io</id>
 <author>
   <name>pi19404</name>
   <email>pi19404@gmail.com</email>
 </author>

 
 <entry>
   <title>Time Delay Estimation Techniques</title>
   <link href="pi19404.github.io/2014/10/13/TDOA1/"/>
   <updated>2014-10-13T00:00:00+05:30</updated>
   <id>pi19404.github.io/2014/10/13/TDOA1</id>
   <content type="html">&lt;h3&gt;Introduction&lt;/h3&gt;

&lt;p&gt;In this article we will look at signal processing techniques for time delay estimation.&lt;/p&gt;

&lt;h3&gt;Background&lt;/h3&gt;

&lt;p&gt;Time delay estimation has been a research topic of significant practical importance in
many fields like radar, sonar, seismology, geophysics, ultrasonic&amp;#39;s, hands-free communications,Doppler positioning systems etc and is of fundamental importance in a variety of signal-processing applications.&lt;/p&gt;

&lt;p&gt;The problem boils down to computing the location of a digitized &amp;quot;signature&amp;quot; waveform residing within a larger time-slice.&lt;/p&gt;

&lt;p&gt;Let $s(n)$ be the sound source signal.
Let us consider two  spatially separated sensors and $x_{1}(t)$ and $x_{2}(t)$ be result of propagation of $s(t)$ through different paths to reach the respective sensors.&lt;/p&gt;

&lt;p&gt;A simple propagation model is that the signals just encounter attenuation and delay and are corrupted by additive noise&lt;/p&gt;

&lt;p&gt;$x_{i}(t) =  a_{i} s(t- \tau_{i}) + b_{i}(t)$&lt;/p&gt;

&lt;p&gt;where ,&lt;/p&gt;

&lt;p&gt;$b_{i}(t)$ is the additive noise uncorrelated with the signal&lt;/p&gt;

&lt;p&gt;We make a assumption that the distance from the sensors to the source is very large compared to the spatial separation between the sensors.Thus signal $s(t)$ received by both the receivers is the same,there is no significant change in the signal as it travels thorough different paths to reach the spatially separated sensors.&lt;/p&gt;

&lt;h3&gt;Cross-correlation&lt;/h3&gt;

&lt;p&gt;One of the simplest method of time delay estimation is cross-correlation .
The cross-correlation of two signals is a measure of similarity between the two sequences.
The cross-correlation function is maximized when both the signals have significant overlap.&lt;/p&gt;

&lt;p&gt;$R_{xy}(k) = \sum_{i} {x(i) y(i+k)} = x[n]*y[-n]$&lt;/p&gt;

&lt;p&gt;Compute maximum absolute value of the correlation function to estimate the lag.&lt;/p&gt;

&lt;p&gt;To test the results we create  create two sequences,one a delayed version of another.
We add white noise to the delayed sequence and use sample correlation to detect the lag.
while performing correlation we normalize the signals,so that correlation measure is bounded
between [0,1]&lt;/p&gt;

&lt;p&gt;Now we increase the noise to try to get estimate of noise co-variance at which this technique fails.
we run the experiment 100 times and estimate the number of times we get the proper answer.&lt;/p&gt;

&lt;h4&gt;Testing with Different Additive Noise Covariance&lt;/h4&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
 *********** Information ************ 
time delay :  10
Noise : 0.1
SNR 15.4390474505
Correct  98  Incorrect  3
mean  9.90099009901 Std  1.0000490136
&lt;/pre&gt;

&lt;p&gt;We see that we are always correct,when the noise levels are low or SNR is high&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://googledrive.com/host/0B-pfqaQBbAAtbExWN0Zya3JySzA/image9.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
 *********** Information ************ 
time delay :  10
Noise : 0.3
SNR 5.89662235611
Correct  48  Incorrect  53
mean  9.79207920792 Std  1.2767287382
&lt;/pre&gt;

&lt;p&gt;The SNR has dropped to about 6db and error in estimation is about 50%. The accuracy is linearly
related with the SNR&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://googledrive.com/host/0B-pfqaQBbAAtbExWN0Zya3JySzA/image10.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
 *********** Information ************ 
time delay :  10
Noise : 0.5
SNR 1.45964736379
Correct  36  Incorrect  65
mean  9.80198019802 Std  1.57969668714

&lt;/pre&gt;

&lt;p&gt;At 2 db SNR we can see the variance of estimated time delay increasing with rising noise levels.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://googledrive.com/host/0B-pfqaQBbAAtbExWN0Zya3JySzA/image11.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;

&lt;p&gt;We can see in the auto-correlation plot the difference between the peak and its neighbors is not significant and depending on the random noise levels introduced,we may not always get the right answer.&lt;/p&gt;

&lt;p&gt;As noise increases,we can see variance in the estimated time delay increases and error in estimation also increases.As the level of noise increases, the uncertainty in the time-delay estimate increases&lt;/p&gt;

&lt;p&gt;We need a reliable time delay estimator in presence of additive noise.&lt;/p&gt;

&lt;p&gt;If is always difficult to estimate the time delays for a base-band signal illustrated above.The due to the noise,we are not reliably able to estimate the delay corresponding to the maximal overlap between the noisy and ideal signal.&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;
def delay(signal,N):
    &quot;&quot;&quot; function introduces a delay of N samples 
    
    Parameters
    -----------
    signal : numpy-array,
             The input signal
             
    N      : integer
             delay
        
    Returns
    --------
    out : numpy-array
          delayed signal
    
    &quot;&quot;&quot;
    d=numpy.zeros((1,N+1));    
    signal=numpy.append(d,signal)
    return signal;

def addNoise(s,variance):
    &quot;&quot;&quot; function add additive white gaussian noise to the input signal 
    
    Parameters
    -----------
    s : numpy-array,
             The input signal
             
    N      : float
             noise covariance
        
    Returns
    --------
    out : numpy-array
          noisy signal    
    
    &quot;&quot;&quot;
    noise = np.random.normal(0,variance,len(s))                    
    s=s+noise;                              
    return s;


if __name__ == &quot;__main__&quot;:  
            
        Fs=1000;
        
        mode=0
        if mode==0:
            x = triang(20);
            x2=x;

        ......
            
        tdelay=10;
        varnoise=0.001;
        loop=1000

        
        #delay the signal
        dx=delay(x,tdelay);
        result=numpy.zeros((1,loop));
        for i in range(loop):
            s=dx;
            #add noise
            s=addNoise(s,varnoise);
            
            #normalize the signals
            s=s/np.linalg.norm(s);
            x1=x2/np.linalg.norm(x2);
            
            unfiltered_signal=s;
            
            r=numpy.correlate(s,x1,mode=&quot;full&quot;)
            arg=np.argmax(r)
            result[0,i]=abs(arg-len(x))     

        #plot the results
        c=np.sum(result==tdelay)
        ic=np.sum(result!=tdelay)
        #10*np.log(np.sum(np.abs(x*x))/
        print &quot; *********** Information ************ &quot;
        print 'time delay : ',tdelay
        print 'Noise :',varnoise
        print &quot;SNR&quot;,10*np.log(np.mean(abs(x*x))/(varnoise*varnoise))/np.log(10);
        print &quot;Correct &quot;,str(c),&quot; Incorrect &quot;,str(ic)
        print &quot;mean &quot;,np.mean(result),&quot;Std &quot;,np.std(result)
        print result
        plt.figure(1)
        subplot(2,2,1) 
        plt.plot(range(len(x)),x)
        xlabel('Time')
        ylabel('Amplitude')

        
        subplot(2,2,2) 
        plt.plot(range(len(unfiltered_signal)),unfiltered_signal)
        xlabel('Time')
        ylabel('Amplitude')

        subplot(2,2,3) 
        plt.plot(range(len(r)),r)
        xlabel('Time')
        ylabel('Amplitude')            
&lt;/pre&gt;
    

&lt;h3&gt;Rectangular Pulse signals&lt;/h3&gt;

&lt;p&gt;Let us look at the results for a different signal in the form of a rectangular pulse .
we will consider the wave of same duration 30.&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
 *********** Information ************ 
time delay :  10
Noise : 0.5
SNR 0.791812460476
Correct  86  Incorrect  15
mean  9.92079207921 Std  1.11411438271
&lt;/pre&gt;

&lt;p&gt;we see that signal has a lower SNR,but accuracy and estimated time delay variance is lower.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://googledrive.com/host/0B-pfqaQBbAAtbExWN0Zya3JySzA/image12.png&quot; alt=&quot;enter image description here&quot;&gt;
Thus the type of pulse we use has a impact on the accuracy of auto-correlation function.&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;
def rectangular(N,start,end):
        &quot;&quot;&quot; fuction generates a rectangular pulse 
        
        Parameters
        ---------
        N : integer
            length of signal
            
        start,end: integer
                starting and ending index of pulse
        
        &quot;&quot;&quot;
        x = np.zeros((1,N))
        x[:,start:end]=1;
        x=x.flatten();    
        return x;
 &lt;/pre&gt;
 
### Coded Pulses 
Broadband  techniques have a sequence of code pulses that increase the accuracy of time delay estimation in the presence of noise.

&lt;pre class=&quot;brush : python &quot;&gt;
time delay :  10
Noise : 0.5
SNR 2.55272505103
Correct  95  Incorrect  6
mean  9.93069306931 Std  1.01725144864

&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;https://googledrive.com/host/0B-pfqaQBbAAtbExWN0Zya3JySzA/image13.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;
x=rectangular(20,8,14)+rectangular(20,2,5)
&lt;/pre&gt;

&lt;p&gt;But most of the time we do not have control of the source pulse.&lt;/p&gt;

&lt;p&gt;In the remainder of the article we will assume that it is a rectangular pulse of duration 1000
with impulse of duration 50 starting at 100.
&lt;pre class=&quot;brush : python &quot;&gt;
time delay :  10
Noise : 0.5
SNR -6.98970004336
Correct  856  Incorrect  144
mean  9.991 Std  0.561176442841
&lt;/pre&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://googledrive.com/host/0B-pfqaQBbAAtbExWN0Zya3JySzA/image14.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;
x=rectangular(1000,100,150)
&lt;/pre&gt;

&lt;p&gt;Now given source signal,we need to see if we can do better in the presence of additive noise atleast.In real life situations there will be a host of other distortions and effects which will  increase the estimation errors apart from noise.&lt;/p&gt;

&lt;p&gt;For pulses like the ones observed above,noise is a dominant factor,signal energy is low compared to noise energy.&lt;/p&gt;

&lt;h3&gt;Modulated Pulses&lt;/h3&gt;

&lt;p&gt;Often the pulses are modulated by sinusoidal waves for longer range transmissions.
&lt;/pre&gt;
time delay :  10
Noise : 0.5
SNR -6.98970004336
Correct  988  Incorrect  12
mean  10.004 Std  0.109471457467
Actual time delay 10
&lt;/pre&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://googledrive.com/host/0B-pfqaQBbAAtbExWN0Zya3JySzA/image15.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;
def sinepulse(N,start,end,f,Fs=1000,tau=0):
        &quot;&quot;&quot; function generates modulated rectangular pulse

        Parameters
        ---------
        N : integer
            length of signal
            
        start,end: integer
                starting and ending index of pulse
                
        f   : integer
              modulated carrier freuency
              
        Fs  : integer
              Sampling freuency

        tau : integer
              carrier phase delay in samples.
              
        Returns
        --------
        out : numpy-array
              modulated rectangular  signal               
          
        &quot;&quot;&quot;
        x = np.zeros((1,N))
        t=np.asarray(range(0,end-start));
        x[:,start:end]=np.cos(2*np.pi*f*(t+tau)/Fs);
        
        return x.flatten();
&lt;/pre&gt;

&lt;h3&gt;Carrier Synchronization Issues&lt;/h3&gt;

&lt;p&gt;we can synchronize exactly with the carrier ,then like broadband techniques we can achieve a significantly enhanced SNR. However synchronizations are never possible.&lt;/p&gt;

&lt;p&gt;we introduce a phase delay of 1 sample to check the effect of carrier phase errors
&lt;pre class=&quot;brush : python &quot;&gt;
*********** Information ************ 
time delay :  10
Noise : 0.3
SNR -2.55272505103
Correct  2  Incorrect  998
mean  229.036 Std  244.973840857&lt;/p&gt;

&lt;p&gt;&lt;/pre&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://googledrive.com/host/0B-pfqaQBbAAtbExWN0Zya3JySzA/image16.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;

&lt;p&gt;In such cases another approach might be to use rectangular envelope
but in the present case that also does not seem to help&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
 *********** Information ************ 
time delay :  10
Noise : 0.3
SNR -2.55272505103
Correct  0  Incorrect  1000
mean  325.336 Std  272.058543523

&lt;/pre&gt;

&lt;h4&gt;Envelope Detection&lt;/h4&gt;

&lt;p&gt;We perform envelope detection on the signal and then apply correlation.
This improves the situation considerably&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
 *********** Information ************ 
time delay :  10
Noise : 0.3
SNR -2.55272505103
Correct  661  Incorrect  339
mean  9.746 Std  0.793400277288
&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;https://googledrive.com/host/0B-pfqaQBbAAtbExWN0Zya3JySzA/image17.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
            if mode==5 or mode ==6 or mode==7:                
                s=abs(scipy.signal.hilbert(s))
&lt;/pre&gt;

&lt;h3&gt;Filtering&lt;/h3&gt;

&lt;p&gt;If we known the carrier frequency ,we can filter the noise outside the signal  bandwidth before performing the correlation operation.&lt;/p&gt;

&lt;p&gt;For the remaining examples we consider signal with carrier frequency of 1KHz and Sampling rate
of 5Khz. The reason for that is explained below&lt;/p&gt;

&lt;p&gt;Let us first look at the frequency characteristics of the rectangular pulse&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://googledrive.com/host/0B-pfqaQBbAAtbExWN0Zya3JySzA/image26.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;

&lt;p&gt;The sharp transition at the edges of the rectangular pulse give rise to high frequency components.
Thus while filtering we need to take these into account,else we may end up distorting the edge
leading to errors in time delay estimation.&lt;/p&gt;

&lt;p&gt;The width of the mainlobe in frequency domain is inversely proportional to width of rectangular pulse
in the time domain.Thus a wider pulse in the time domain will provide higher spectral compression in the frequency domain.&lt;/p&gt;

&lt;p&gt;we can see that main lobe of the rectangular pulse would have normalize freuency of 0.05
Few of the side lobes also contain significant information.&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;

def plotFrequency(b,a=1):
    &quot;&quot;&quot; the function plots the frequency and phase response &quot;&quot;&quot;
    w,h = signal.freqz(b,a)
    h_dB = abs(h);#20 * np.log(abs(h))/np.log(10)
    subplot(211)
    plot(w/max(w),h_dB)
    #plt.ylim(-150, 5)
    ylabel('Magnitude (db)')
    xlabel(r'Normalized Frequency (x$\pi$rad/sample)')
    title(r'Frequency response')
    subplot(212)
    h_Phase = np.unwrap(np.arctan2(np.imag(h),np.real(h)))
    plot(w/max(w),h_Phase)
    ylabel('Phase (radians)')
    xlabel(r'Normalized Frequency (x$\pi$rad/sample)')
    title(r'Phase response')
    plt.subplots_adjust(hspace=0.5)
&lt;/pre&gt;    

&lt;p&gt;We try with a normalized frequency components of 0.05 which contains around 3 adjacent side-lobes&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://googledrive.com/host/0B-pfqaQBbAAtbExWN0Zya3JySzA/image31.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;

&lt;p&gt;we can see that the signal has been attenuated near the edges.Thus the distortion in the region where the pulse starts will lead  to ambiguity of time delay computation.&lt;/p&gt;

&lt;p&gt;Thus we need to consider the freuency components to retain so that sharp transition in time domain are not affected&lt;/p&gt;

&lt;p&gt;The carrier freuency is 1Khz .The normalized carrier frequency at sampling rate of 5Khz is 0.4.
Thus the spectrum of modulated rectangular pulse is centered at 0.4&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://googledrive.com/host/0B-pfqaQBbAAtbExWN0Zya3JySzA/image30.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;

&lt;p&gt;Thus significant frequency band lies from 0.3 to 0.5.&lt;/p&gt;

&lt;p&gt;If we bandpass filter frequencies in this band,we should get a relatively noiseless waveform,which is expected to improve the performance of correlation.&lt;/p&gt;

&lt;p&gt;We apply band-pass  butter worth filter of order 2 with normalized cutoff frequencies are 0.2 and 0.6.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://googledrive.com/host/0B-pfqaQBbAAtbExWN0Zya3JySzA/image33.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;
def butter_bandpass(lowcut, highcut, fs, order=5):
    &quot;&quot;&quot; function returns the bandpass butterworth filter coefficients 
    
    Parameters
    -------------    
    lowcut,highcut : integer
                     lower and higher cutoff freuencies in Hz
                     
    Fs : Integer
         Samping freuency in Hz

    order : Integer
            Order of butterworth filter                     
        
    Returns
    --------
    b,a - numpy-array
          filter coefficients 
          
    &quot;&quot;&quot;
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low,high], btype='bandpass')
    return b, a
    
    
def bandpass_filter(data, lowcut, highcut, fs, order,filter_type='butter_worth'):

 &quot;&quot;&quot; the function performs bandpass filtering 
    
    Parameters
    -------------
    data : numpy-array
           input signal
           
    lowcut,highcut : integer
                     lower and higher cutoff freuencies in Hz
                     
    Fs : Integer
         Samping freuency in Hz

    order : Integer
            Order of butterworth filter                     
        
    Returns
    --------
    out : numpy-array
          Filtered signal
    
    &quot;&quot;&quot;
    global once
    if filter_type=='butter_worth':
        b, a = butter_bandpass(lowcut, highcut, fs, order=order)            
        if once==0:
            plt.figure(2)
            plotFrequency(b,a)
            once=1
        y = filtfilt(b, a, data)
        return y
    
&lt;/pre&gt;    

&lt;p&gt;The butter-worth band-pass filter is not a zero phase .It will introduce different phase delay depending of frequency. The.The phase plot of the band pass filter is not linear,&lt;/p&gt;

&lt;p&gt;To see the phase delay effects of bandpass butter-worth filter,we consider the case of no noise .
we can see that instead of 10,delay is introduced as 12 due to butter-worth filter. However we have a stable singular correlation peak being detected&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
 *********** Information ************ 
time delay :  10
Noise : 0.001
SNR 46.9897000434
Correct  0  Incorrect  1000
mean  12.0 Std  0.0
&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;https://googledrive.com/host/0B-pfqaQBbAAtbExWN0Zya3JySzA/image34.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;

&lt;pre class=&quot;brush:python&quot;&gt;
.....
        #introduce delay
        dx=delay(x,tdelay);
        result=numpy.zeros((1,loop));
        for i in range(loop):
            s=dx;

            #add noise            
            if varnoise!=0:
                s=addNoise(s,varnoise)

            #normalize signals                
            s=s/np.linalg.norm(s);
            x1=x2/np.linalg.norm(x2);
            if carrier!=None:
                unfiltered_signal=s;
                if mode==1:
                    s = bandpass_filter(s, carrier-500, carrier+500, Fs,order,filter_type)
                if mode==2:
                    s = bandpass_filter(s, carrier-750, carrier+750, Fs,order,filter_type)

                filtered_signal=s;
            else:
                unfiltered_signal=s;
             
            #perform envelope detection
            s=abs(scipy.signal.hilbert(s))
            if mode==1 or mode==2:
                r=numpy.correlate(s,x1,mode=&quot;full&quot;)

.......
&lt;/pre&gt;                

&lt;p&gt;To compensate for that we need to use zero phase filtering techniques like forward backward filtering.&lt;/p&gt;

&lt;p&gt;To achieve zero phase the linear filter is applied twice, once forward and once backwards. The combined filter has zero phase.In general  forward-backward filtering squares the amplitude response and zeros the phase response if phase of filter is linear.&lt;/p&gt;

&lt;p&gt;we see that there are few errors inspire of applying the forward backward algorithm due to distortions introduced by bandpass filtering of high frequency components.Though the mean is closer to actual delay of 10.&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
 *********** Information ************ 
time delay :  10
Noise : 0
Correct  0  Incorrect  1000
mean  9.0 Std  0.0
&lt;/pre&gt;

&lt;p&gt;This tells us that we cannot eliminate the effects introduced due to filtering.&lt;/p&gt;

&lt;p&gt;we now introduce noise and compare the performance against envelope detection.
&lt;pre class=&quot;brush : python &quot;&gt;
 *********** Information ************ 
time delay :  10
Noise : 0.3
SNR -2.55272505103
Correct  442  Incorrect  558
mean  9.49 Std  0.678159273327
&lt;/pre&gt;
&lt;pre class=&quot;brush : python &quot;&gt;
 *********** Envelope Detection ************ 
time delay :  10
Noise : 0.3
SNR -2.55272505103
Correct  661  Incorrect  339
mean  9.746 Std  0.793400277288
&lt;/pre&gt;&lt;/p&gt;

&lt;p&gt;Compared to just envelope detection,we can see that standard deviation has reduced.Thus the neighborhood of estimated TDOA values are reduced,though the mean is shifted away from 10 may be due to the phase delay effects of bandpass filter.&lt;/p&gt;

&lt;p&gt;Though band-pass filtering did not lead to a significant improvement  due no the noise within the frequency bandwidth .It is essential to keep out unwanted frequency components due to environmental noise and other factors.&lt;/p&gt;

&lt;h3&gt;Code&lt;/h3&gt;

&lt;p&gt;The code used in the article can be found in github repository &lt;a href=&quot;https://github.com/pi19404/pyVision/tree/master/pySignalProc&quot;&gt;&amp;quot;Github Link&amp;quot;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Files &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;TDOA1.py&lt;/li&gt;
&lt;li&gt;TODA2.py - Band Pass Filtering &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All the plots included in the article can generated by changing the mode variable in the files&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Discrete Fourier Transform For Freuency Analysis</title>
   <link href="pi19404.github.io/2014/10/11/DFT1/"/>
   <updated>2014-10-11T00:00:00+05:30</updated>
   <id>pi19404.github.io/2014/10/11/DFT1</id>
   <content type="html">&lt;h3&gt;Introduction&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Fourier analysis&lt;/strong&gt; is generally concerned with the analysis and synthesis of functions. The decomposition of signal into easy-to-analyze components and the reconstruction from such components.&lt;/p&gt;

&lt;p&gt;In this article we will look at Fourier analysis of discrete time signals.&lt;/p&gt;

&lt;p&gt;Discrete time signal are defined at only particular set of time instances and are represented as sequence of real numbers that have continuous range of values&lt;/p&gt;

&lt;h3&gt;Fourier Series Representation of signal&lt;/h3&gt;

&lt;p&gt;A discrete time complex exponential is periodic in nature&lt;/p&gt;

&lt;p&gt;$ e^{j k\omega (n+N)} = e ^{j k \omega n} $
where  , $\omega=\frac{2\pi}{N}$&lt;/p&gt;

&lt;p&gt;The set of all discrete time time complex exponential signals that are periodic with period N is given by
$\phi_{k}[n] =  e ^{j k \frac{2\pi}{N} n} , \forall k \in \mathcal{Z}$&lt;/p&gt;

&lt;p&gt;$$ \phi_{k+rN}[n] = e ^{j (k+rN) \frac{2\pi}{N} n}  = e ^{j k \frac{2\pi}{N} n}  * e ^{j r N \frac{2\pi}{N} n}   $$
$$ \phi_{k+rN}[n] = e ^{j k \frac{2\pi}{N} n} e ^{j r  2\pi n} =e ^{j k \frac{2\pi}{N} n}$$&lt;/p&gt;

&lt;p&gt;When k is changed by integral multiples of N ,we get identical sequence.
Thus there are only N unique values of k for which we can define unique discrete time complex exponential sequence.&lt;/p&gt;

&lt;p&gt;$$ \begin{align} x[n]=\sum_{k} x[k] e^{j k \omega n} \end{align}$$&lt;/p&gt;

&lt;p&gt;$e^{j k \omega n} $ are only unique over N successive values of k,summation is only considered
over this range&lt;/p&gt;

&lt;p&gt;$$ \begin{align} x[n]=\sum_{k={N}} x[k] e^{j k \omega n} \end{align}$$&lt;/p&gt;

&lt;p&gt;$x[n]$ is  periodic with period N.&lt;/p&gt;

&lt;h3&gt;Discrete time Fourier Series&lt;/h3&gt;

&lt;p&gt;Let us consider a arbitrary periodic signal $x[n]$ with period N and that  $x[n]$ can be expressed in the form&lt;/p&gt;

&lt;p&gt;$$ \begin{align} x[n]=\sum_{k} X[k] \phi_{k}[n] \end{align}$$&lt;/p&gt;

&lt;p&gt;Thus we need to find out if $x[n]$ can be expressed in the above form and if so the coefficients $x[k]$ for which this representation is valid.&lt;/p&gt;

&lt;p&gt;The theoretical derivations for the same can be found in all signal processing references.The result are as follows&lt;/p&gt;

&lt;p&gt;$$ \begin{align} X[k] =\sum_{n=0}^{N-1}x[n] e^{- j k \frac{2\pi}{ N} n } \end{align}$$ and 
$$\begin{align} x[n] =\frac{1}{N}\sum_{k=0}^{N-1}X[k]\,e^{j k \frac{2\pi}{ N} n } \end{align} $$&lt;/p&gt;

&lt;p&gt;$X[k]$ are called as &lt;strong&gt;Fourier series coefficients&lt;/strong&gt;.The coefficients are also referred to as spectral coefficients of signal $x[n]$.&lt;/p&gt;

&lt;p&gt;The representation of signal $x[n]$ in terms of spectral  coefficients is called as the &lt;strong&gt;Fourier series representation&lt;/strong&gt; of the signal&lt;/p&gt;

&lt;p&gt;These coefficient specify the decomposition of signal $x[n]$ into sum of N harmonically related complex exponential .&lt;/p&gt;

&lt;p&gt;Any periodic discrete time signal $x[n]$ can be represented using the Fourier series representation 
and Fourier series representation enable us to represent any periodic signal as weighted sum of complex exponential s.&lt;/p&gt;

&lt;p&gt;Let us look at some examples to understand what information can Fourier series representation of a signal give us.&lt;/p&gt;

&lt;pre class=&quot;brush: python&quot;&gt;
def FourierSeries(input,N=None):
    &quot;&quot;&quot; computes the fourier series coefficients of input signal
    
    Parameters
    -----------
    input : numpy array
            input discrete time signal
            
    Returns
    --------
    out : complex,list
          fourier series coefficients
    &quot;&quot;&quot;
    
    N=len(input);

    w=2*cmath.pi/N;
    input=input[0:N];
    n=numpy.arange(0,N);    
    r=cexp(-1j*w*n);

    output = [complex(0)] * N    
    for k in range(N):        
        r=input*cexp(-1j*w*n*k)          
        output[k]=np.sum(r);
        
   
    return output;

&lt;/pre&gt;

&lt;p&gt;Let us consider a sinusoidal signal with frequency $f_{s}=10Hz$,for a duration of 1sec
Lets sample this waveform at $F_{s}=150Hz$ and observe the Fourier series coefficients of the signal&lt;/p&gt;

&lt;p&gt;we sample at 150Hz,In the frequency spectrum,just consider a single period of the cosine waveform,ie 
$F_{s}/f_{s}=15$ samples.The output can be seen in subplot 2&lt;/p&gt;

&lt;p&gt;we take complete signal,and compute the Fourier series coefficients,output can be seen in subplot 3&lt;/p&gt;

&lt;p&gt;Let us consider  magnitude plot of Fourier series&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://googledrive.com/host/0B-pfqaQBbAAtbExWN0Zya3JySzA/image2.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
def FourierSinusoids(F,w,Fs,synthesis=None):    
    &quot;&quot;&quot; the function generates a discrete time sinusoid,computes
    the Fourier series coefficients and plots the time and frequency
    data 
 
     Paraneters
     ----------
     F : numpy array
         sinuoidal frequency components
         
     w  : numpy array
          the weights associated with freuency components
         
     Fs : Integer
          sampling frequency
          
     synthesis : int
                 if 1 ,reconstructs signal and plots the original and reconstructed
                 signal
 
    &quot;&quot;&quot;
    if synthesis==None:
        synthesis=0;
        
    Ts=1.0/Fs;   
    xs=numpy.arange(0,1,Ts) 
    
    signal=numpy.zeros(np.shape(xs));
    for i in range(len(F)):
        omega=2*np.pi*F[i];
        signal = signal+ w[i]*numpy.cos(omega*xs);
    #plot the time domain signal    
    subplot(2,1,1)
    plt.plot(range(0,len(signal)),signal)
    xlabel('Time')
    ylabel('Amplitude')
    title('time doman')
    #plt.ylim(-2, 2)
    
    #compute fourier series coefficients
    r1=FourierSeries(signal)
    a1=cabs(r1)
    
    if synthesis==0:
        #plot the freuency domain signal
        L=len(a1);
        fr=np.arange(0,L);
        subplot(2,1,2)
        plt.stem(fr,a1,'r') # plotting the spectrum
        xlabel('Freq (Hz)')
        ylabel('|Y(freq)|')
        title('complete signal')
        ticks=np.arange(0,L+1,25);
        plt.xticks(ticks,ticks);     
        show() 
        
    if synthesis==1:
        rsignal=IFourierSeries(r1);
        print np.allclose(rsignal, signal)    
        subplot(2,1,2) 
        plt.stem(xs,signal)
        xlabel('Time')
        ylabel('Amplitude')
        title('reconstructed signal')
        show() 


if __name__ == &quot;__main__&quot;:     

    mode =0
    
    if mode==0:
        F=[10];
        F=np.array(F);
        w=numpy.ones(F.shape);
        #plot the time domain signal and fourier series component
        FourierSinusoids(F,w,150);
&lt;/pre&gt;

&lt;p&gt;we can see that the frequency corresponding to 10Hz we can observe peak.The total number of Fourier
series coefficients are equal to the total number of input samples.&lt;/p&gt;

&lt;p&gt;Now let us consider a combination of sinusoidal signals at freuency 10 and 15Hz.The signal is periodic with frequency which is LCM of 10 and 15 ie 5Hz.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://googledrive.com/host/0B-pfqaQBbAAtbExWN0Zya3JySzA/image4.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
    if mode==1:
        F=[10,20];
        F=np.array(F);
        w=numpy.ones(F.shape);
        FourierSinusoids(F,w,150); 
&lt;/pre&gt;

&lt;p&gt;Now lets keep on adding frequency components.The below signal is periodic with period of 1 sec &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://googledrive.com/host/0B-pfqaQBbAAtbExWN0Zya3JySzA/image5.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;

    if mode==2:
        F=range(1,10);
        F=np.array(F);
        w=numpy.ones(F.shape);      
        FourierSinusoids(F,w,150);    
&lt;/pre&gt;

&lt;h3&gt;Synthesis of Signal&lt;/h3&gt;

&lt;p&gt;In the earlier section we saw the process of decomposition of aperiodic signal into its frequency components.&lt;/p&gt;

&lt;p&gt;we will now look at the &lt;strong&gt;synthesis of signal&lt;/strong&gt; from its Fourier series coefficients.&lt;/p&gt;

&lt;p&gt;$${x[n] =
\frac{1}{N}\sum_{k=0}^{N-1}X[k]\,e^{j k \frac{2\pi}{ N} n }}$$&lt;/p&gt;

&lt;p&gt;Below is plot of the original and reconstructed signal and python code for the &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://googledrive.com/host/0B-pfqaQBbAAtbExWN0Zya3JySzA/image6.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
def IFourierSeries(input):
    &quot;&quot;&quot; function reconstructs the signal from fourier series coefficients
    
    Parameters
    ---------
    input : cmath list
            fourier series coefficients    
    
    Returns
    -----------
    out : numpy arrary
          reconstructed signal
    
    &quot;&quot;&quot;
    N=len(input);
    w=2*cmath.pi/N;
    k=numpy.arange(0,N);    
    output = [complex(0)] * N   
    for n in range(N):  
        r=input*cexp(-1j*w*n*k);
        output[n]=np.mean(r);

    print output.__class__    
    return output;


    if mode==3:
        F=range(1,10);
        F=np.array(F);
        w=numpy.ones(F.shape);
        FourierSinusoids(F,w,150,1); 
&lt;/pre&gt;

&lt;h3&gt;Discrete  Fourier Transform&lt;/h3&gt;

&lt;p&gt;Now arises the situation what do we do for a-periodic signals.After a lot of theorotical analysis
on Discrete time Fourier transform and sampling in the frequency domain,it turns out
we just assume periodic extension of aperiodic signal and compute Fourier series as above.&lt;/p&gt;

&lt;p&gt;The Fourier series coefficients for a periodic signal are also periodic with same period N&lt;/p&gt;

&lt;p&gt;$$X[k+N]=X[k]$$&lt;/p&gt;

&lt;p&gt;If we consider a single period of N values of Fourier series coefficient ,we obtain a finite duration sequence
which is called Discrete Fourier Transform (DFT).&lt;/p&gt;

&lt;p&gt;Thus by computing the DFT we obtain the Fourier series coefficients for single period.&lt;/p&gt;

&lt;p&gt;It is upto us to choose a period of the signal.Let us consider a aperiodic impulse of length 150 and on-duty cycle of 5.&lt;/p&gt;

&lt;p&gt;Let us consider N=150,450 and observe the results.
&lt;img src=&quot;https://googledrive.com/host/0B-pfqaQBbAAtbExWN0Zya3JySzA/image7.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;

&lt;p&gt;At we increase the period of the signal we can see that resolution in the freuency domain increases.&lt;/p&gt;

&lt;p&gt;As $N \rightarrow \infty$ ,the samples in the freuency domain will be placed closer and closer .
If samples in frequency domain are spaced infinitely closely,it can be considered a continuous signal.
This gives us Discrete Time Fourier Transform representation of the signal.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://googledrive.com/host/0B-pfqaQBbAAtbExWN0Zya3JySzA/image8.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
def FourierRect(N):     
        &quot;&quot;&quot; the function generates rectangular aperiodic pulse and computer the DFT coefficients
        
        Parameters
        ----------
        N : period of aperiodic signal
             
        &quot;&quot;&quot;
        x = np.zeros((1,N))
        x[:,0:30]=1;
        x=x.flatten();
    
        
        #compute the DFT coefficients
        r1=FourierSeries(x)
        #magnitude of DFT coefficients
        a1=cabs(r1)

        #plot the time domain signal
        subplot(2,1,1)
        plt.plot(range(0,len(x)),x)
        xlabel('Time')
        ylabel('Amplitude')
        title('time doman')
        plt.ylim(-2,2);
        
        #plot the DFT coefficients
        L=len(a1);
        fr=np.arange(0,L);
        subplot(2,1,2)
        plt.stem(fr,a1,'r') # plotting the spectrum
        xlabel('Freq (Hz)')
        ylabel('|Y(freq)|')
        title('complete signal')
        ticks=np.arange(0,L+1,25);
        plt.xticks(ticks,ticks);     
        show() 

    if mode==4:
       FourierRect(150);

    if mode==5:
       FourierRect(150*3);        
&lt;/pre&gt;

&lt;p&gt;It samples in frequency domain are spaced infinitely closely,it can be considered a continuous signal.
This representation of is called as Fourier Transform.&lt;/p&gt;

&lt;p&gt;$$ X(k) = \sum x[n] exp(-j\omega k n)  $$&lt;/p&gt;

&lt;p&gt;distance between adjacent samples is $\omega$ .As $ w \rightarrow 0 $ samples are placed infinitely closely with each other .&lt;/p&gt;

&lt;p&gt;$$ X(w) = \sum x[n] exp(-j\omega  n)  $$&lt;/p&gt;

&lt;p&gt;Fourier transform is continuous in nature and cannot be used for numeral computation .&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Discrete Fourier transform&lt;/strong&gt; is sampled version of Discrete Time Fourier transform of a  signal and in in a form that is suitable for numerical computation on a signal processing unit.&lt;/p&gt;

&lt;p&gt;A fast Fourier transform (FFT) is an algorithm to compute the discrete Fourier transform (DFT) and its inverse.It is a efficient way to compute the DFT of a signal.&lt;/p&gt;

&lt;p&gt;we will use the  python FFT routine can compare the performance with naive implementation&lt;/p&gt;

&lt;p&gt;Using the inbuilt FFT routine :Elapsed time was 6.8903e-05 seconds&lt;/p&gt;

&lt;p&gt;Using the naive code :Elapsed time was 0.0653119 seconds&lt;/p&gt;

&lt;p&gt;we can see improvement of order of 1000&lt;/p&gt;

&lt;p&gt;The naive algorithm has complexity of $o(N^2)$ and FFT algorithm as complexity of $O(N log N)$&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
    if mode==6:
        Fs=150;
        F=range(1,10);
        F=np.array(F);        
        w=numpy.ones(F.shape);

        
        Ts=1.0/Fs;   
        xs=numpy.arange(0,1,Ts) 
    
        signal=numpy.zeros(np.shape(xs));
        for i in range(len(F)):
            omega=2*np.pi*F[i];
            signal = signal+ w[i]*numpy.cos(omega*xs);        
            
            
        start_time = time.time()
        FourierSeries(signal)
        end_time = time.time()
        print(&quot;Elapsed time naive algo  %g seconds&quot; % (end_time - start_time)) 

        start_time = time.time()
        fft(signal)
        end_time = time.time()
        print(&quot;Elapsed time of fft algo  %g seconds&quot; % (end_time - start_time)) 
&lt;/pre&gt;      

&lt;h4&gt;Circular Convolution and DFT&lt;/h4&gt;

&lt;h5&gt;Circular Shift of a sequence&lt;/h5&gt;

&lt;p&gt;Let $x[n]$ denote the finite length time domain sequence&lt;/p&gt;

&lt;p&gt;Once we take DFT ,in time domain we are constructing the periodic extension of the signal
Thus a time shit of signal is actually implies circular shit of the signal.&lt;/p&gt;

&lt;p&gt;$$x[n] \Leftrightarrow X[k] $$
$$x[(n-n_{o})_{N}]  \Leftrightarrow e^{-j k \omega n_{o}} X[k]$$ &lt;/p&gt;

&lt;p&gt;One of the most basic application in signal processing is linear convolution.&lt;/p&gt;

&lt;p&gt;It can be used to represents the output of discrete time LTI system,correlation and cross correlation,
filtering and host of other signal processing operations.&lt;/p&gt;

&lt;p&gt;Linear Convolution in discrete time system is represented as&lt;/p&gt;

&lt;p&gt;$$y[n]=\sum_{i} x[i] h[n-i]$$&lt;/p&gt;

&lt;p&gt;The signals $x[n]$ and $h[n]$ are finite duration discrete time signals .&lt;/p&gt;

&lt;p&gt;Linear convolution of 2 sequences of length M,L  results in sequence of length M+L-1&lt;/p&gt;

&lt;p&gt;An associated discrete time Fourier transform property &lt;/p&gt;

&lt;p&gt;$$Y\left(\Omega\right) = X\left(\Omega\right)H\left(\Omega\right)$$&lt;/p&gt;

&lt;p&gt;Let us consider 2 sequences,take their DFT,multiply them and then take the inverse
&lt;strong&gt;&lt;code&gt;Circular convolution in time domain leads to multiplication of DFT coefficients in the frequency domain&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;lets take the seuences &lt;code&gt;[1,1,1,1,1]&lt;/code&gt; and &lt;code&gt;[1,2,3,4]&lt;/code&gt;&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
The result of linear convolution is `[ 1  3  6 10 10  9  7  4 ]`
The result of inverse of product of DFT coefficients is `[ 10.  10.  10.  10.  10.]`
&lt;/pre&gt;

&lt;p&gt;we take N point DFT of signals,N=max(M,L)&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
...|1 1 1 1 1|1 1 1 1 1|1 1 1 1 1|....
...|1 2 3 4 0|1 2 3 4 0|1 2 3 4 0|....
Result 
...|10|10|10| ...
&lt;/pre&gt;

&lt;p&gt;Taking the DFT leads to periodicity in time domain,now we perform convolution as usual.
the $h[n-n_{o}]$ will be circularly shifted&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
...|1 1 1 1 1|1 1 1 1 1|1 1 1 1 1|....
...|0 1 2 3 4|0 1 2 3 4|0 1 2 3 4|....

Result 
...|10|10|10| ...
&lt;/pre&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
...|1 1 1 1 1|1 1 1 1 1|1 1 1 1 1|....
...|4 0 1 2 3 |4 0 1 2 3|4 0 1 2 3|....

Result 
...|10|10|10| ...
&lt;/pre&gt;

&lt;p&gt;After N shifts we are repeating the sequence
Thus we perform the convolution of N shifts equivalent to the period of sequence in time domain&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
Circular convolution
[ 10.  10.  10.  10.  10.]
&lt;/pre&gt;

&lt;p&gt;Circular shift  is consequence of periodicity introduced by the DFT  and results in circular convolution operation&lt;/p&gt;

&lt;p&gt;We can consider this due to the time aliasing being introduced due to sampling in the frequency domain.
The sampling theorem states that to avoid aliasing in frequency domain the sampling rate must be greater than twice the maximum frequency of signal being sampled.&lt;/p&gt;

&lt;p&gt;We had mentioned earlier that DFT is the sampled version of DTFT .The question arises that how do we increase the sampling rate to avoid time domain aliasing.&lt;/p&gt;

&lt;p&gt;We saw that zero padding the sequence leads to samples of Fourier series are placed more closely together.Equivalent to saying increases the sampling rate of DTFT in frequency domain,&lt;/p&gt;

&lt;p&gt;This gives us a intuition that if we zero pad the sequence,it will lead to increased sampling rate of the DTFT of the signal.&lt;/p&gt;

&lt;p&gt;For the result of circular convolution to be equal to the linear convolution,we simply zero pad the sequences to length of M+L-1.&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
...|1 1 1 1 1 0 0 0 0 |1 1 1 1 1 0 0 0 0 |
...|1 2 3 4 0 0 0 0 0 |1 2 3 4 0 0 0 0 0 |

result : 10

...|1 1 1 1 1 1 0 0 0 |1 1 1 1 1 0 0 0 0 |
...|0 1 2 3 4 0 0 0 0 |0 1 2 3 4 0 0 0 0 |

result : 10

...|1 1 1 1 1 1 0 0 0 |1 1 1 1 1 0 0 0 0 |
...|0 0 1 2 3 4 0 0 0 |0 0 1 2  3 4 0 0 0 |

result : 9
&lt;/pre&gt;

&lt;p&gt;thus we can see that due to zero padding,the circular shifted components are zeros
and the result obtained is equivalent to linear convolution result.&lt;/p&gt;

&lt;p&gt;The code for the above example is given below&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
    if mode == 7:
        x=np.array([1,1,1,1,1]);
        h=np.array([1,2,3,4,0]);
        r=np.convolve(x,h)
        print 'Linear convolution'
        print r

        #take 5 point DFT of the sequence
        f1=fft(x,5);
        f2=fft(h,5);
        s=ifft(f1*f2);
        print 'Circular convolution'
        print abs(s)
        
        #take 9 point DFT of sequence
        f1=fft(x,9);
        f2=fft(h,9);
        s=ifft(f1*f2);
        print 'zero padded Circular convolution'
        print r
&lt;/pre&gt;
        

&lt;h3&gt;Code&lt;/h3&gt;

&lt;p&gt;The code for all the examples can be found at github repository
&lt;a href=&quot;https://github.com/pi19404/pyVision/tree/master/pySignalProc/tutorial/fourierSeries.py&quot;&gt;https://github.com/pi19404/pyVision/tree/master/pySignalProc/tutorial/fourierSeries.py&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You can change the mode variable in the file from 1-7 to generate all the plots shown in the article&lt;/p&gt;

&lt;p&gt;The file fourierSeries.py can also be downloaded from below link&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://www.codeproject.com/KB/Articles/828166/pySignalProc.zip&quot;&gt;Download Link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Multilayer Perceptron in Python</title>
   <link href="pi19404.github.io/2014/10/03/test/"/>
   <updated>2014-10-03T00:00:00+05:30</updated>
   <id>pi19404.github.io/2014/10/03/test</id>
   <content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In this article we will look at supervised learning algorithm called Multi-Layer Perceptron (MLP) and implementation of single hidden layer MLP&lt;/p&gt;

&lt;h3&gt;Perceptron&lt;/h3&gt;

&lt;p&gt;A perceptron is a  unit that computes a single output from multiple real-valued inputs by forming a linear combination according to its input weights and then possibly putting the output through some nonlinear function called the activation function&lt;/p&gt;

&lt;p&gt;Below is a figure illustrating the operation of perceptron&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://googledrive.com/host/0B-pfqaQBbAAtbExWN0Zya3JySzA/images1.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://encrypted-tbn3.gstatic.com/images?q=tbn:ANd9GcSPBshuqpGJBgvzx9ECppUv6QBg7ipgPH4XDEle3gZVn3Ku56MT&quot;&gt;figure taken from&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The output of perceptron can be expressed as &lt;/p&gt;

&lt;p&gt;$f(x) = G( W^T x+b)$&lt;/p&gt;

&lt;p&gt;$x$ is the input vector 
$(W,b)$ are the parameters of perceptron 
 $f$ is the non linear function&lt;/p&gt;

&lt;h3&gt;Multi Layer Perceptron&lt;/h3&gt;

&lt;p&gt;The MLP network consists of input,output and hidden layers.Each hidden layer consists of numerous perceptron&amp;#39;s which are called hidden units&lt;/p&gt;

&lt;p&gt;Below is figure illustrating a feed forward neural network architecture for Multi Layer perceptron&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://googledrive.com/host/0B-pfqaQBbAAtbExWN0Zya3JySzA/mlp1.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;a href=&quot;http://www.deeplearning.net/tutorial/_images/mlp.png&quot;&gt;(figure taken from)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A single-hidden layer MLP  contains a array of perceptrons .
The output of hidden layer of MLP can be expressed as a function &lt;/p&gt;

&lt;p&gt;$f(x) = G( W^T x+b)$&lt;/p&gt;

&lt;p&gt;$f: R^D \rightarrow R^L$, 
where D is the size of input vector $x$ 
$L$ is the size of the output vector
$G$ is activation function.&lt;/p&gt;

&lt;p&gt;In case the activation function G is a sigmoid function then a single-layer MLP consisting of just the output layer is equivalent to a logistic classifier         &lt;/p&gt;

&lt;p&gt;$\begin{align} f_{i}(x)=\frac{e^{W_{i}x+b_{i}}}{\sum_{j} e^{W_{j}x+b_{j}}} \end{align}$&lt;/p&gt;

&lt;p&gt;Each unit of input layer corresponds to element of input vector.
Each output unit of logistic classifier generate a prediction probability that input vector belong to a specified class.&lt;/p&gt;

&lt;h2&gt;Feed Forward Neural Network&lt;/h2&gt;

&lt;p&gt;Let us first consider the most classical case of a single hidden layer neural network&lt;/p&gt;

&lt;p&gt;The number of inputs to hidden layer is $(d)$ and number of outputs of hidden layer are $(m)$
The hidden layer performs mapping  of vector of dimensionality $d$ to vector of dimensionality $m$.&lt;/p&gt;

&lt;p&gt;Each unit of hidden layer of a MLP can be parameterized by a  weight matirx and bias vector  $(W,b)$ and a activation function $(\mathcal{G})$.The output of a hidden layer is activation function applied to linear combination of input and weight vector.&lt;/p&gt;

&lt;p&gt;Dimensionality of weight matrix and bias vector are determined by desired number of output units.
If the number of  inputs to hidden layer/dimensionality of input is $\mathcal{M}$ and number of outputs is $\mathcal{N}$ then dimensionality of weight vector in $\mathcal{NxM}$ and that of  bias vector is $\mathcal{N}x1$.&lt;/p&gt;

&lt;p&gt;We can consider that hidden layer consists of $\mathcal{N}$ hidden units ,each of which accepts a $\mathcal{M}$ dimensional vector and produces a single output.&lt;/p&gt;

&lt;p&gt;The output is the affine transformation of the input layer followed by the appplication of function $f(x)$ ,which is typically a non linear function like sigmoid of inverse tan hyperbolic function.&lt;/p&gt;

&lt;p&gt;The vector valued function  $h(x)$  is the output of the hidden layer.&lt;/p&gt;

&lt;p&gt;$$ h(x) = f(W^T x + c ) $$&lt;/p&gt;

&lt;p&gt;The output layer of MLP is typically Logistic regresson classifier,if probabilistic outputs are desired for classification purposes in which case the activation function is the softmax regression function.&lt;/p&gt;

&lt;h3&gt;Single Hidden Layer Multi Layer Perceptron&amp;#39;s&lt;/h3&gt;

&lt;p&gt;Let ,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$h_{i-1}$ denote the input vector to the i-th  layer&lt;/li&gt;
&lt;li&gt;$h_{i}$ denote the output vector of the i-th layer.&lt;/li&gt;
&lt;li&gt;$h_{0}$=x is vector that represents input layer &lt;/li&gt;
&lt;li&gt;$h_{n}=y$ is output layer which produces the desired prediction output.&lt;/li&gt;
&lt;li&gt;$f(x)$ denote the activation function &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thus we denote the output of each hidden layer as&lt;/p&gt;

&lt;p&gt;$h_{k}(x) = f(b_{k} + w_{k}^T h_{i-1}(x)) = f(a_{k}) $&lt;/p&gt;

&lt;p&gt;Considering sigmoid activation function,gradient of fundtion wrt arguments can be written as&lt;/p&gt;

&lt;p&gt;$\begin{align} \frac{\partial \mathbf{h}_{k}(x)  }{\partial \mathbf{a}_{k}}=  f(a_{k})(1- f(a_{k})) \end{align}$ &lt;/p&gt;

&lt;p&gt;The computation associated with each hidden unit $(i)$ of the layer can be denoted as&lt;/p&gt;

&lt;p&gt;$$h_{k,i}(x) = f(b_{k,i} + W_{k,i}^T h_{i-1}(x)) = f(a_{k}(x))$$&lt;/p&gt;

&lt;p&gt;The output layer is a Logistic regression classifier.The output is a probabilistic output denoting the confident that input belongs to the predicted class.The cost function defined for the same is defined as negative log likelyhood over the training data&lt;/p&gt;

&lt;p&gt;$$L = -log (p_{y}) $$&lt;/p&gt;

&lt;p&gt;The idea is to maximize $p_{y}= P( Y =y_{i} | x )$ as estimator of conditional probability of the class $y$ given that input is $x$.This is the cost function for training algorithm.&lt;/p&gt;

&lt;h3&gt;Back-Propagation Algorithm&lt;/h3&gt;

&lt;p&gt;The Back-Propagation Algorithm is recursive gradient algorithm used to optimize the parameters MLP wrt to defined loss function.Thus our aim is that each layer of MLP the hidden units are computed so that cost function is maximized.&lt;/p&gt;

&lt;p&gt;Like in logistic regression we compute the gradients of weights wrt to the cost function . The gradient of the cost function wrt all the weights in various hidden layers are computed.Standard gradient based optimization is performed to obtain the parameters that will minimize the likelihood function.&lt;/p&gt;

&lt;p&gt;The output layer determines the cost function.Since we are using Logistic regression as output layer.The cost function is the softmax function.Let L denote the cost function.&lt;/p&gt;

&lt;p&gt;There is nothing different we do in backpropagation algorithm that any other optimization techniue.The aim is to determine how the weights and biases change in the network &lt;/p&gt;

&lt;p&gt;$ \begin{align} \frac{\partial L}{\partial W_{k,i,j} } \text{ and } \frac{\partial L}{\partial b_{k,i,j} } \end{align}$.&lt;/p&gt;

&lt;h4&gt;output layer&lt;/h4&gt;

&lt;p&gt;$\begin{align} L = -log ( f(a_{k,i}) ) \end{align}$&lt;/p&gt;

&lt;p&gt;$\begin{align} \frac{\partial L  }{\partial \mathbf{a}_{k,i}} = \frac{\partial L  }{\partial \mathbf{h}_{k,i}} \frac{\partial \mathbf{h}_{k,i} }{\partial \mathbf{a}_{k,i}} = -\frac{1}{h_{k,i}} * h_{k,i}*(1-h_{k,i}) = (h_{k,i}-1)\end{align}  $&lt;/p&gt;

&lt;p&gt;$ \begin{align} \frac{\partial L  }{\partial \mathbf{a}_{k,i}} =\mathbf{h}_{k,j} - 1_{y=y_{i}} \end{align}$&lt;/p&gt;

&lt;p&gt;The above expression can be considered as the error in output.When $y=y_{i}$ the error is $(1-p_{i})$ and then $y \ne y_{i}$ the error in prediction is $p_{i}$.&lt;/p&gt;

&lt;h4&gt;hidden layer&lt;/h4&gt;

&lt;p&gt;$\begin{align}\frac{\partial L }{\partial \mathbf{a}_{k-1,j}} =  \frac{\partial L }{\partial \mathbf{h}_{k-1,j}} \frac{\partial \mathbf{h}_{k-1,j} }{\partial \mathbf{a}_{k-1,j}} \end{align}$&lt;/p&gt;

&lt;p&gt;Thus the idea is to start computing gradients from the bottom most layer.To compute the gradients of the cost function wrt parameters at the i-th layer we need to know the gradients of cost function wrt parameters at $(i+1)$th layer.&lt;/p&gt;

&lt;p&gt;We start with gradient computation at the logistic classifier level.The propagate backwards,updating the parameters at each layer&lt;/p&gt;

&lt;p&gt;Let us consider the case of other other hidden layers&lt;/p&gt;

&lt;p&gt;$\begin{align} \frac{\partial L }{\partial \mathbf{h}_{k-1,j}} = \sum_{i} \frac{\partial L }{\partial \mathbf{a}_{k,i}}\frac{\partial \mathbf{a}_{k,i} }{\partial \mathbf{h}_{k-1,j}} = \sum_{i} \frac{\partial L }{\partial \mathbf{a}_{k,i}} W_{k,i,j}  \end{align} $&lt;/p&gt;

&lt;p&gt;The implementation of the above equation &lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
    def linear_gradient(self,weights,error):   
            &quot;&quot;&quot; The function compues gradient of likelihood function wrt output of hidden layer
            :math:`\\begin{align} \\frac{\partial L }{\partial \mathbf{h}\_{k-1,j}} \\end{align}`
            
            Parameters 
            ------------
            weights : ndarray,shape=(n_out,n_hidden)
                      weights of next hidden layer, :math:`\\begin{align} \mathbf{W}\_{k,i,j}  \\end{align}`
                      
            error   : ndarray,shape=(n_out,)
                      backpropagated error from next layer :math:`\\begin{align} \\frac{\partial L }{\partial \mathbf{a}\_{k,i}} \\end{align}`
        
            Returns 
            -----------     
            out : ndarray,shape=(n_hidden,)                
                  compute the backpropagated error, :math:`\\begin{align} \\frac{\partial L }{\partial \mathbf{h}\_{k-1,j}} \\end{align}`
            &quot;&quot;&quot;            
            
            return numpy.dot(error,weights);
&lt;/pre&gt;

&lt;p&gt;The gradients computation of parameters of hidden layers is as follows&lt;/p&gt;

&lt;p&gt;$\begin{align}\frac{\partial L }{\partial \mathbf{W}_{k-1,i,j}} =  \frac{\partial L }{\partial \mathbf{a}_{k-1,j}} \frac{\partial \mathbf{a}_{k-1,j} }{\partial \mathbf{W}_{k-1,i,j}}=\frac{\partial L }{\partial \mathbf{a}_{k-1,j}} \mathbf{h}_{k-2,j} \end{align}$&lt;/p&gt;

&lt;p&gt;$\begin{align}\frac{\partial L }{\partial \mathbf{b}_{k-1,i}} =  \frac{\partial L }{\partial \mathbf{a}_{k-1,i}} \frac{\partial \mathbf{a}_{k-1,i} }{\partial \mathbf{b}_{k-1,i}}=\frac{\partial L }{\partial \mathbf{a}_{k-1,i}}  \end{align}$&lt;/p&gt;

&lt;p&gt;This is implemented as below ,where the input&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$x$ represents $\begin{align} \frac{\partial \mathbf{h}_{k,j} }{\partial \mathbf{a}_{k,j}} \end{align}$ -output gradient&lt;/li&gt;
&lt;li&gt;$y$ represents $\begin{align} h_{k-2,j} \end{align}$ -activation&lt;/li&gt;
&lt;li&gt;$w$ represents $\begin{align} \frac{\partial L }{\partial \mathbf{a}_{k-1,i}}\end{align}$ -error&lt;/li&gt;
&lt;/ul&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
   
    def compute_error(self,x,w,y):      
        &quot;&quot;&quot;                 
        function computes the gradient of the likelyhood function wrt to parameters  of the hidden layer for single input
        

        Parameters 
        -------------
        x : ndarray,shape=(n_hidden,)

        w : ndarray,shape=(n_hidden,)
            `w` represents :math:`\\begin{align} \\frac{\partial L }{\partial \mathbf{h}\_{k,i}}\end{align}` the gradient of the likelyhood fuction wrt output of hidden layer
            
        y : ndarray,shape=(n_in,)
            `y` represents :math:`\mathbf{h}\_{k-2,j}` the input hidden layer
        
        Returns
        ------------
        res : ndarray,shape=(n_in+1,n_hidden)        
              :math:`\\begin{align} \\frac{\partial L }{\partial \mathbf{W}\_{k-1,i,j}}  \\text{ and } \\frac{\partial L }{\partial \mathbf{W}\_{k-1,i}} \end{align}`
        &quot;&quot;&quot;        
       
        
        x=x*w;                
        #gradient of likelyhood function wrt input activation
        res1=x.reshape(x.shape[0],1);
        #gradient of likelyhood function wrt weight matrix
        res=np.dot(res1,y.reshape(y.shape[0],1).T);
        self.eta=0.0001
        #code for L1 and L2 regularization 
        if self.Regularization==2:
           res=res+self.eta*self.W;
        if self.Regularization==1:
           res=res+self.eta*np.sign(self.W);

        #stacking the parameters and preparing for returning            
        res=np.hstack((res,res1));
        return res.T;


    def cost_gradients(self,weights,activation,error):        
        &quot;&quot;&quot; function to compute the gradient of log 
        likelyhood function wrt the parameters of the hidden layer
        averaged over all the input samples.        
        
        Parameters 
        -------------
        weights : numpy,shape(n_out,n_hidden),
                  weight matrix of the next layer,W\_{k,i,j} 
                  
                  
        activation: numpy,shape=(N,n_in)
                    input to the hidden layer \mathbf{h}\_{k-2,j}
                    
        error : numpy,shape=(n_out,) 
                 \frac{\partial L }{\partial \mathbf{a}\_{k,i}}
        
        Returns
        
        -------------
        gW : ndarray,shape=(n_hidden,n_in+1)
             coefficient parameter matrix of next hidden layer,
             :math:`\\begin{align} \\frac{\partial L }{\partial \mathbf{W}\_{k-1,i,j}}  \\text{ and } \\frac{\partial L }{\partial \mathbf{W}\_{k-1,i}} \end{align}`
        &quot;&quot;&quot;                                       
        we=self.linear_gradient(weights,error)
        ag=self.activation_gradient()
        e=[ self.compute_error(a,we,b) for a,b in izip(ag,activation)]
        gW=np.mean(e,axis=0).T        
        return gW;        
&lt;/pre&gt;

&lt;p&gt;Once we have the gradients and have computed the new parameters,the &lt;code&gt;function update&lt;/code&gt; is called to updated the new parameters in the model.&lt;/p&gt;

&lt;p&gt;This function is called by the Optimizer module that performs SGD based optimizations,all the optimization parameters like learning rate are handled by the optimizer methods.&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
    def update_parameters(self,params):
        &quot;&quot;&quot; function to updated the learn parameters to the model
        
        Parameters
        ----------
        grads : ndarray,shape=(n_hidden,n_in+1)        
                coefficient parameter matrix                
        
        &quot;&quot;&quot;
        
        self.params=params;
        param1=self.params.reshape(-1,self.nparam);
        self.W=param1[:,0:self.nparam-1];
        self.b=param1[:,self.nparam-1];
        
&lt;/pre&gt;

&lt;h3&gt;Implementation Details&lt;/h3&gt;

&lt;p&gt;The class HiddenLayer encapsulates all the methods for prediction,classification,training,gradient computation and error propagation that are required&lt;/p&gt;

&lt;p&gt;The important attributes of the HiddenLayer class are &lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;
    Attributes        
    -----------
    `out` : array-like ,shape=[n_out]
    The output of hidden layer 
    
    `params`:array-like ,shape=[n_out,n_in+1]        
     parameters of hidden layer
    
    `W,b`:array-like,shape=[n_out,n_int],shape=[n_out,1]
     parameters in the form of weight matrix and bias vector characterizing 
     the hidden layer
     
     `activation`:function
     the non linear activation function
     
    .. note :
    in the below functions to n_hidden denotes the number of output units of present hidden layer
    n_out denotes the number of output units of next hidden layer
    and n_in denotes the size of input vector to present hidden layer
    
    def compute(self,input):
        &quot;&quot;&quot;function computes the output of the hidden layer for input matrix
      
        Parameters
        ----------
        input   :   ndarray,shape=(N,n_in)
                    :math:`h\_{i-1}(x)` is the `input`

        Returns
        -----------
        output  : ndarray ,shape=(N,n_out)
                    :math:`f(b_k + w_k^T h\_{i-1}(x))` ,affine transformation over input
        &quot;&quot;&quot;                
        #performs affine transformation over input vector        
        linout=numpy.dot(self.W,input.T)+np.reshape(self.b,(self.b.shape[0],1));     
        #applies non linear activation function over computed linear transformation
        self.output=self.activation(linout).T;                 
        return self.output;

&lt;/pre&gt;

&lt;p&gt;A class MLP  encapsulates all the methods for prediction,classification,training,forward and back propagation,saving and loading models etc.
Below 3 important functions are displayed.The learn function is called at every optimizer loop.
This calls the forward and backward iteration methods and updated the parameters of each hidden layer&lt;/p&gt;

&lt;p&gt;the forward iteration simply computes the output of network and while propagate_backward fuctions
is responsible for passing suitable inputs and weights to each hidden layer so that it can execute the backward algorithm loop&lt;/p&gt;

&lt;pre class=&quot;brush : python &quot;&gt;

               
   def propagate_backward(self,error,weights,input):                 
        &quot;&quot;&quot; the function that executes the backward propagation loop on hidden layers
                
        Parameters 
        ----------------
        error : numpy array,shape=(n_out,)
                average prediction error over all the input samples in output layer
                :math:`\\begin{align}\frac{\partial L  }{\partial \mathbf{a}\_{k,i}} \\end{align}`


        weight : numpy array,shape=(n_out,n_hidden)        
                 parameter weight matrix of the output layer
        
        
        input : ndarray,shape=(n_samples,n_in)
                input training data
        Returns
        ----------------
        None 
        
        &quot;&quot;&quot;              


        #input matrix for the hidden layer    
        input1=input;
        for i in range(self.n_hidden_layers):                        
            prev_error=np.inf;
            best_grad=[];
            for k in range(1):
                &quot;&quot;&quot; computing the derivative of the parameters of the hidden layers&quot;&quot;&quot;
                hidden_layer=self.hiddenLayer[self.n_hidden_layers-i-1];
                hidden_layer.compute(input1);
          
                # computing the gradient of likelyhood function wrt the parameters of the hidden layer 
                grad=hidden_layer.cost_gradients(weights,input1,error);
                #update the parameter of hidden layer
                res=self.update(hidden_layer.params,grad.flatten(),0.13);
            
                &quot;&quot;&quot; update the parameters &quot;&quot;&quot;
                hidden_layer.update_parameters(res);
            #set the weights ,inputs and error required for the back propagation algorithm
            #for the next layer
            weights=hidden_layer.W;
            error=grad[:,hidden_layer.n_in];                                    
            self.hiddenLayer[self.n_hidden_layers-i-1]=hidden_layer;
            input1=hidden_layer.output;

   def propagate_forward(self,input):
       &quot;&quot;&quot;the function that performs forward iteration to compute the output
        
       Parameters
       -----------
       input : ndarray,shape=(n_samples,n_in)
               input training data
       
       &quot;&quot;&quot;
       self.predict(input)

                
  
   def learn(self,update):
        &quot;&quot;&quot; the main function that performs learning,computing gradients and updating parameters 
            this is called by the optimizer module for each iteration
        
        Parameters
        ----------
        update - python function
                 this represents the update function that performs the gradient descent iteration
        &quot;&quot;&quot;
        #set the training data
        x,y=self.args;
        #set the update function
        self.update=update;                        
        #execute the forward iteration loop
        self.propagate_forward(x)  
        #set the input for output layer
        args1=(self.hidden_output,y);
        #set the input for the output logistic regression layer
        self.logRegressionLayer.set_training_data(args1);
        #gradient computation and parameter updation of output layer
        [params,grad]=self.logRegressionLayer.learn(update);
        self.logRegressionLayer.update_params(params);
       
        #initialize the gradiients and weights for backward error propagation
        error=grad;
        weights=self.logRegressionLayer.W;
        
        #perform the backward iteration over the hidden layers
        if self.n_hidden_layers &gt;0:   
             weights=self.logRegressionLayer.W;
             self.propagate_backward(error,weights,x)
             
        return [None,None];                        
&lt;/pre&gt;

&lt;h3&gt;Selecting the parameters of the model&lt;/h3&gt;

&lt;p&gt;As mentioned earlier that MLP consits of input,hidden and output layers.There is not fixed rule to determine the number of hidden units.The parameters are application specific and best parameters are often arrived at by emperical testing process.Less number of hidden units leads to increased generalization and training error while having a large number of training units leads to issues of with training of large number of parameters and significantly large training time.&lt;/p&gt;

&lt;h3&gt;Issues with MLP&lt;/h3&gt;

&lt;p&gt;One of the issues observed in MLP training is the slow nature of learning.The below figure illustrates the nature of learning process when a small learning parameter or improper regularization constant is chosen.Various adaptive methods can be implemented which can improve the performance ,but slow convergence and large learning times is an issue with Neural networks based learning algorithms.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://googledrive.com/host/0B-pfqaQBbAAtbExWN0Zya3JySzA/save.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;

&lt;h3&gt;Code&lt;/h3&gt;

&lt;p&gt;The important files related to MLP are&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;MLP.py&lt;/li&gt;
&lt;li&gt;LogisticRegression.py&lt;/li&gt;
&lt;li&gt; Optimizer.py&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The latest version of the code can be found in github repository &lt;a href=&quot;https://www.github.com/pi19404/pyVision&quot;&gt;www.github.com/pi19404/pyVision&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The files used in the current article can be downloaded from below link
 - &lt;a href=&quot;https://github.com/pi19404/pyVision/archive/pyVision_alpha0.002.zip&quot;&gt;Github Release&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The dataset and model file can be found under the models and data repository&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;MLP.pyvision - model file&lt;/li&gt;
&lt;li&gt;mnist.pkl.gz - data file&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;make suitable changes to the path in MLP.py file before running the code.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Jekyll A Static Website Generator</title>
   <link href="pi19404.github.io/2014/10/03/Jekyll/"/>
   <updated>2014-10-03T00:00:00+05:30</updated>
   <id>pi19404.github.io/2014/10/03/Jekyll</id>
   <content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In this article we will look at &lt;strong&gt;Jekyll&lt;/strong&gt; static site generator to generate a static website and host the same on &lt;strong&gt;github&lt;/strong&gt;  using github pages.&lt;/p&gt;

&lt;h2&gt;Background&lt;/h2&gt;

&lt;p&gt;Jekyll is a simple, blog aware, static site generator. &lt;/p&gt;

&lt;p&gt;A static site generator is a utility that generates ready-to-publish static HTML pages  from a set of files usually in markdown or HTML which are suitable for deployment directory on any web-server .The blog-aware means that it can support and maintain website with content added in series like that of blogs.&lt;/p&gt;

&lt;p&gt;Jekyll is the engine behind GitHub Pages, which enables us to use Jekyll to host  the project’s page, blog, or website from GitHub’s servers for free&lt;/p&gt;

&lt;h2&gt;Github Pages and Jekyll Installation&lt;/h2&gt;

&lt;p&gt;Let us consider the github project &lt;a href=&quot;https://github.com/pi19404/pyVision&quot;&gt;www.github.com/pi19404/pyVision.&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;Github pages generator gives us the facility to create a website for the project using one of the standard defined themes .This is the easiest way to start a project without having to worry about the HTML themes and CSS files etc and just focus on content writing.&lt;/p&gt;

&lt;p&gt;To generate a static webpage of the project go to project settings on github.com
&lt;img src=&quot;https://googledrive.com/host/0B-pfqaQBbAAtbExWN0Zya3JySzA/ss5.png&quot; alt=&quot;enter image description here&quot;&gt;
Click on the &lt;strong&gt;&lt;em&gt;Automatic page generator&lt;/em&gt;&lt;/strong&gt; options&lt;/p&gt;

&lt;p&gt;This will provide you with a markdown editor where you can enter the contents for the main page of the website.&lt;/p&gt;

&lt;p&gt;Enter the name ,content and other details and click on &lt;strong&gt;&lt;em&gt;&amp;quot;Continue to Layouts Button&amp;quot;&lt;/em&gt;&lt;/strong&gt; to proceed to HTML theme selection
&lt;img src=&quot;https://googledrive.com/host/0B-pfqaQBbAAtbExWN0Zya3JySzA/ss6.png&quot; alt=&quot;enter image description here&quot;&gt;
Select the desired theme and click on &lt;strong&gt;&lt;em&gt;&amp;quot;Publish Page&amp;quot;&lt;/em&gt;&lt;/strong&gt; button to complete the process
&lt;img src=&quot;https://googledrive.com/host/0B-pfqaQBbAAtbExWN0Zya3JySzA/ss7.png&quot; alt=&quot;enter image description here&quot;&gt;
This will lead to github servers hosting the created website on &amp;quot;http://pi19404.github.io/pyVision&amp;quot;&lt;/p&gt;

&lt;p&gt;now we are ready to modify and edit the website.The sources of the website are also stored in github repository of the  project in the gh-pages branch.&lt;/p&gt;

&lt;p&gt;Thus all we have to do to access the sources is to checkout the gh-pages branch and start modifying content on local server and then push changes onto remote github repository.The github servers will regenerated the website and latest changes will be reflected on your website.&lt;/p&gt;

&lt;p&gt;The same process can be used to maintain blogs,post or any other content of the website.&lt;/p&gt;

&lt;p&gt;Every GitHub Page is run through Jekyll when you push content to gh-pages branch within your repository&lt;/p&gt;

&lt;h2&gt;Installing Jekyll&lt;/h2&gt;

&lt;p&gt;Though Jekyll installation on local PC is not necessary.It can be installed in order to preview the website  and troubleshoot issues or bugs before pushing the site on GitHub Pages.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Ruby - Jekyll requires the Ruby version 1.9.3 or higher.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;To install ruby easiest way is to download ruby install managers like &lt;strong&gt;&lt;em&gt;&amp;quot;rvm - The Ruby Version Manager&amp;quot;&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Detailed installation instruction for &lt;code&gt;rvm&lt;/code&gt; can be found at &lt;a href=&quot;http://rvm.io/rvm/install&quot;&gt;http://rvm.io/rvm/install&lt;/a&gt;  or install the software from package manager like synaptic&lt;/li&gt;
&lt;li&gt;To install Ruby : &lt;code&gt;rvm install 1.9.3&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;This will download the ruby 1.9.3 sources and perform compilation and deployment  at &amp;quot;/usr/share/ruby-rvm/&amp;quot;&lt;/li&gt;
&lt;li&gt;In some cases you many encounter compilation issues due to outdated version of OpenSSL,in which case install the openssl from rvm tool : &lt;code&gt;rvm pkg install openssl&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;This will install the openssl packaged within the rvm installation directory&lt;/li&gt;
&lt;li&gt;&lt;p&gt;While compiling ruby we can given commandline arguments so that it referes the openSSL package from the rvm install directory and not default system path :&lt;/p&gt;

&lt;p&gt;&lt;code&gt;rvm install 1.9.3 --with-openssl-dir=/usr/share/ruby-rvm/usr&lt;/code&gt; &lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Bundler - Bundler is a package manager ,This can be installed as easily&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;gem install bundler&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Jekyll installation&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Clone the sites repository on the local machine&lt;/li&gt;
&lt;li&gt;Change the branch to gh-pages if you have created a project website&lt;/li&gt;
&lt;li&gt;Create a file called GemFile in the directory with the following content&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre class=&quot;brush : html &quot;&gt;
                 source 'https://rubygems.org'
                 gem 'github-pages'
&lt;/pre&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt; - Run the command : `bundle install` for installing Jekyll
 - To ensure that local development environment is same as that of github regularily update the local environment 

   `bundle update github-pages `
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Running Jekyll : &lt;code&gt;bundle exec jekyll serve&lt;/code&gt;
The website is accessible for preview at : http://localhost:4000&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Layouts and Website Template&lt;/h2&gt;

&lt;p&gt;Front-matter is just a set of metadata, delineated by three dashes which takes for form of valid YAML content.Any file that contains a front matter block will be processed by Jekyll as a special file.
Thus .Jekyll requires that Markdown files have front-matter defined at the top of every file.The front matter can be included in top of markdown or HTML files.&lt;/p&gt;

&lt;p&gt;Between the dashed lines you can set predefined variables ( title,layout) or set custom used defined variables.&lt;/p&gt;

&lt;p&gt;The present article was written using Markdown syntax and frontmatter included on top of the files was&lt;/p&gt;

&lt;pre class=&quot;brush : html &quot;&gt;
---
layout: post
title: Jekyll A Static Website Generator
---
Introduction
-------------
In this article we will look at **Jekyll** static site generator to generate a static website and host the same on **github**  using github pages.
&lt;/pre&gt;

&lt;p&gt;The layout tag specifies the template to be used for generating posts.The templates can be makrdown containing HTML contents.&lt;/p&gt;

&lt;p&gt;Jekyll uses the &lt;a href=&quot;https://github.com/shopify/liquid/wiki/liquid-for-designers&quot;&gt;Liquid template system&lt;/a&gt; .The variables defined in front matter and page contents can be accessed  accessed using the Liquid tags both within the files as well as any layouts that page of post relies on.&lt;/p&gt;

&lt;p&gt;Let us look at the template for post called &lt;strong&gt;post.html&lt;/strong&gt; and how contents are incorporated using Liquid markup language&lt;/p&gt;

&lt;h2&gt;&lt;pre class=&quot;brush : html &quot;&gt;&lt;/h2&gt;

&lt;h2&gt;layout: default&lt;/h2&gt;

&lt;p&gt;&lt;article class=&quot;post&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;h1&gt;{{  page.title }}&lt;/h1&gt;&lt;/p&gt;

&lt;p&gt;&lt;div class=&quot;entry&quot;&gt;
    {{  content }}&amp;#39;
  &lt;/div&gt;&lt;/p&gt;

&lt;p&gt;&lt;div class=&quot;date&quot;&gt;
    Written on {{  page.date | date: &amp;quot;%B %e, %Y&amp;quot; }}
  &lt;/div&gt;&lt;/p&gt;

&lt;p&gt;&lt;/article&gt;
&lt;/pre&gt;&lt;/p&gt;

&lt;p&gt;Thus when a post is created using the template,then the title specified in the frontmatter is the post is accessed via variable page.title.The page content is accessed via variable content.
Jekyll provides numerous  predefined global variables that you can set in the front matter of a page or post.&lt;/p&gt;

&lt;p&gt;Information on some of them can be found at &lt;a href=&quot;http://jekyllrb.com/docs/frontmatter/#predefined-global-variables&quot;&gt;Frontmatter Predefined variables&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The rendered html output for above file is &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://googledrive.com/host/0B-pfqaQBbAAtbExWN0Zya3JySzA/ss8.png&quot; alt=&quot;enter link description here&quot;&gt;&lt;/p&gt;

&lt;p&gt;we have seen that in &lt;strong&gt;post.html&lt;/strong&gt; file we have included frontmatter &lt;strong&gt;&amp;quot;layout&amp;quot;&lt;/strong&gt;.This enables us to include the contents of the post.html file into another files as its contents using Liquid markup language.&lt;/p&gt;

&lt;p&gt;This enables us to maintain layout and content files separately and we can change the site layout whenever required without making any changes to the content files.The frontmatter predefined variable provides a lot of flexibility in how we can define complex layouts and themes for the website.&lt;/p&gt;

&lt;p&gt;There is a &lt;strong&gt;index.html&lt;/strong&gt; file in the project repository that is auto generated by github pages.
We modify this file so that it can be used as a base template for all the pages on the website.&lt;/p&gt;

&lt;p&gt;we create a &lt;code&gt;_layouts&lt;/code&gt; directory in the root folder of the repository.This directory contains all the files that can be accessed by defining the &lt;code&gt;layout&lt;/code&gt; variable in the frontmatter of the files.If the layout variable is assigned values post then the file post.html in the _layout directory will be accessed.&lt;/p&gt;

&lt;p&gt;we create a file default.html.The default.html file contains the html headers,javascript,stylesheets etc as well as contents to be included in header and footer of pages. Again Liquid markup language is used to specify where the content is be be included&lt;/p&gt;

&lt;pre class=&quot;brush : html &quot;&gt;
&lt;html&gt;
  &lt;head&gt;
    &lt;meta charset=&quot;utf-8&quot;&gt;
    &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;chrome=1&quot;&gt;
    &lt;title&gt;pyVision by pi19404&lt;/title&gt;

    &lt;link rel=&quot;stylesheet&quot; href=&quot;/stylesheets/styles.css&quot;&gt;
    &lt;link rel=&quot;stylesheet&quot; href=&quot;/stylesheets/pygment_trac.css&quot;&gt;
    &lt;script src=&quot;javascripts/scale.fix.js&quot;&gt;&lt;/script&gt;
    &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1, user-scalable=no&quot;&gt;

 
  &lt;/head&gt;
  &lt;body&gt;
    &lt;div class=&quot;wrapper&quot;&gt;
      &lt;header&gt;
         ........ 
      &lt;/header&gt;
      &lt;section&gt;
        {{  content }}  &lt;!-----  contents inserted here ---&gt;
      &lt;/section&gt;
    &lt;/div&gt;
    &lt;footer&gt;
      .......
    &lt;/footer&gt;
    
    
  &lt;/body&gt;
&lt;/html&gt;
&lt;/pre&gt;

&lt;p&gt;If the file is markdown file then all its contents are inserted in place of contents tag.If the file is HTML then the declaration inside section tag of class content is inserted in place of contents tag.All the pages of the website including the main page index.html contains frontmatter are designed so that generated contents are inserted within default layout.&lt;/p&gt;

&lt;h2&gt;Posting Blog&lt;/h2&gt;

&lt;p&gt;All the blog posts reside in the &lt;code&gt;_posts&lt;/code&gt; directory.The format of filename is &lt;strong&gt;year-month-day-title.ext&lt;/strong&gt;.This will generate the blogs posts in year/month/day directory of static website.
The blog posts can be html or markdown .&lt;/p&gt;

&lt;p&gt;Let us create a blog post called 2014-10-03-Jekyll.md&lt;/p&gt;

&lt;pre class=&quot;brush : html &quot;&gt;
---
layout: post
title: Jekyll A Static Website Generator
---

Introduction
-------------
In this article we will look at **Jekyll** static site generator to generate a static website and host the same on **github**  using github pages.
..........
&lt;/pre&gt;

&lt;p&gt;Now we need to provide links to access the blog content from the main page of website.This is done using Jekyll variables and adding the below content in the &lt;strong&gt;index.html&lt;/strong&gt; page&lt;/p&gt;

&lt;pre class=&quot;brush : html &quot;&gt;
---
layout: default
---
&lt;section class=&quot;content&quot;&gt;
      &lt;section&gt;
        .......
      &lt;/section&gt;

&lt;ul class=&quot;entries&quot;&gt;
  &lt;li&gt; Blog Posts -{{  site.url }}&lt;/li&gt;
  {{%  for post in site.posts %}}

  &lt;li&gt;
    &lt;a href=&quot;{{&quot;&gt;      
      &lt;h3&gt;{{ post.title }}&lt;/h3&gt;
    &lt;/a&gt;
  &lt;/li&gt;
 
  {{%  endfor %}}
&lt;/ul&gt;

&lt;/section&gt;
&lt;/pre&gt;

&lt;p&gt;now we launch the website on local machine by executing command &lt;/p&gt;

&lt;p&gt;&lt;code&gt;bundle exec jekyll serve&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://googledrive.com/host/0B-pfqaQBbAAtbExWN0Zya3JySzA/ss9.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/p&gt;

&lt;p&gt;now we push the repository onto github &lt;/p&gt;

&lt;p&gt;&lt;code&gt;git push origin gh-pages&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;access the webpage and observe the similar output as on local server &lt;/p&gt;

&lt;p&gt;The markdown used is called &lt;code&gt;GitHub Flavored Markdown&lt;/code&gt;,It is different from standard markdown language.The following like gives the difference and highlights the features of &lt;a href=&quot;https://help.github.com/articles/github-flavored-markdown/&quot;&gt;Github Flavored markdown&lt;/a&gt; language&lt;/p&gt;

&lt;p&gt;now that we have created the html,say we want to use the html content on other sites like codeproject or blogger.we can access the html at _site/posts/2013/10/03/Jekyll.html.&lt;/p&gt;

&lt;p&gt;we can copy the relevant sections of html file and with slight modifications make it compatible with other websites.&lt;/p&gt;

&lt;h2&gt;Code&lt;/h2&gt;

&lt;p&gt;The pyVision repository can be found at &lt;code&gt;[www.github.com/pi19404/pyvision](www.github.com/pi19404/pyvision)&lt;/code&gt; and website can be seen at &lt;a href=&quot;http://pi19404.github.io/pyVision/&quot;&gt;pyvision&lt;/a&gt;
All the files used in the preset article can be found in the gh-pages branch of the repository.&lt;/p&gt;

&lt;p&gt;The file for the present article can be found at &lt;strong&gt;_posts/2014-10-03-Jekyll.md&lt;/strong&gt;
The source files in &lt;code&gt;gh-pages branch&lt;/code&gt; of the repository files can also be downloaded from  &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://www.codeproject.com/KB/Articles/826515/pyVision.rar&quot;&gt;Download pyVision.rar&lt;/a&gt; - 1.5 MB&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.codeproject.com/KB/Articles/826515/pyVision.zip&quot;&gt;Download pyVision.zip&lt;/a&gt; - 1.6 MB&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/pi19404/pyVision/releases/download/pyVision/pyVision_ghpages.rar&quot;&gt;Alternate link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Introducing Lanyon</title>
   <link href="pi19404.github.io/2014/01/02/introducing-lanyon/"/>
   <updated>2014-01-02T00:00:00+05:30</updated>
   <id>pi19404.github.io/2014/01/02/introducing-lanyon</id>
   <content type="html">&lt;p&gt;Lanyon is an unassuming &lt;a href=&quot;http://jekyllrb.com&quot;&gt;Jekyll&lt;/a&gt; theme that places content first by tucking away navigation in a hidden drawer. It&amp;#39;s based on &lt;a href=&quot;http://getpoole.com&quot;&gt;Poole&lt;/a&gt;, the Jekyll butler.&lt;/p&gt;

&lt;h3&gt;Built on Poole&lt;/h3&gt;

&lt;p&gt;Poole is the Jekyll Butler, serving as an upstanding and effective foundation for Jekyll themes by &lt;a href=&quot;https://twitter.com/mdo&quot;&gt;@mdo&lt;/a&gt;. Poole, and every theme built on it (like Lanyon here) includes the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Complete Jekyll setup included (layouts, config, &lt;a href=&quot;/404&quot;&gt;404&lt;/a&gt;, &lt;a href=&quot;/atom.xml&quot;&gt;RSS feed&lt;/a&gt;, posts, and &lt;a href=&quot;/about&quot;&gt;example page&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Mobile friendly design and development&lt;/li&gt;
&lt;li&gt;Easily scalable text and component sizing with &lt;code&gt;rem&lt;/code&gt; units in the CSS&lt;/li&gt;
&lt;li&gt;Support for a wide gamut of HTML elements&lt;/li&gt;
&lt;li&gt;Related posts (time-based, because Jekyll) below each post&lt;/li&gt;
&lt;li&gt;Syntax highlighting, courtesy Pygments (the Python-based code snippet highlighter)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Lanyon features&lt;/h3&gt;

&lt;p&gt;In addition to the features of Poole, Lanyon adds the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Toggleable sliding sidebar (built with only CSS) via &lt;strong&gt;☰&lt;/strong&gt; link in top corner&lt;/li&gt;
&lt;li&gt;Sidebar includes support for textual modules and a dynamically generated navigation with active link support&lt;/li&gt;
&lt;li&gt;Two orientations for content and sidebar, default (left sidebar) and &lt;a href=&quot;https://github.com/poole/lanyon#reverse-layout&quot;&gt;reverse&lt;/a&gt; (right sidebar), available via &lt;code&gt;&amp;lt;body&amp;gt;&lt;/code&gt; classes&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/poole/lanyon#themes&quot;&gt;Eight optional color schemes&lt;/a&gt;, available via &lt;code&gt;&amp;lt;body&amp;gt;&lt;/code&gt; classes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/poole/lanyon#readme&quot;&gt;Head to the readme&lt;/a&gt; to learn more.&lt;/p&gt;

&lt;h3&gt;Browser support&lt;/h3&gt;

&lt;p&gt;Lanyon is by preference a forward-thinking project. In addition to the latest versions of Chrome, Safari (mobile and desktop), and Firefox, it is only compatible with Internet Explorer 9 and above.&lt;/p&gt;

&lt;h3&gt;Download&lt;/h3&gt;

&lt;p&gt;Lanyon is developed on and hosted with GitHub. Head to the &lt;a href=&quot;https://github.com/poole/lanyon&quot;&gt;GitHub repository&lt;/a&gt; for downloads, bug reports, and features requests.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Example content</title>
   <link href="pi19404.github.io/2014/01/01/example-content/"/>
   <updated>2014-01-01T00:00:00+05:30</updated>
   <id>pi19404.github.io/2014/01/01/example-content</id>
   <content type="html">&lt;div class=&quot;message&quot;&gt;
  Howdy! This is an example blog post that shows several types of HTML content supported in this theme.
&lt;/div&gt;

&lt;p&gt;Cum sociis natoque penatibus et magnis &lt;a href=&quot;#&quot;&gt;dis parturient montes&lt;/a&gt;, nascetur ridiculus mus. &lt;em&gt;Aenean eu leo quam.&lt;/em&gt; Pellentesque ornare sem lacinia quam venenatis vestibulum. Sed posuere consectetur est at lobortis. Cras mattis consectetur purus sit amet fermentum.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Curabitur blandit tempus porttitor. Nullam quis risus eget urna mollis ornare vel eu leo. Nullam id dolor id nibh ultricies vehicula ut id elit.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Etiam porta &lt;strong&gt;sem malesuada magna&lt;/strong&gt; mollis euismod. Cras mattis consectetur purus sit amet fermentum. Aenean lacinia bibendum nulla sed consectetur.&lt;/p&gt;

&lt;h2&gt;Inline HTML elements&lt;/h2&gt;

&lt;p&gt;HTML defines a long list of available inline tags, a complete list of which can be found on the &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/HTML/Element&quot;&gt;Mozilla Developer Network&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;To bold text&lt;/strong&gt;, use &lt;code&gt;&amp;lt;strong&amp;gt;&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;To italicize text&lt;/em&gt;, use &lt;code&gt;&amp;lt;em&amp;gt;&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Abbreviations, like &lt;abbr title=&quot;HyperText Markup Langage&quot;&gt;HTML&lt;/abbr&gt; should use &lt;code&gt;&amp;lt;abbr&amp;gt;&lt;/code&gt;, with an optional &lt;code&gt;title&lt;/code&gt; attribute for the full phrase.&lt;/li&gt;
&lt;li&gt;Citations, like &lt;cite&gt;&amp;mdash; Mark otto&lt;/cite&gt;, should use &lt;code&gt;&amp;lt;cite&amp;gt;&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;del&gt;Deleted&lt;/del&gt; text should use &lt;code&gt;&amp;lt;del&amp;gt;&lt;/code&gt; and &lt;ins&gt;inserted&lt;/ins&gt; text should use &lt;code&gt;&amp;lt;ins&amp;gt;&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Superscript &lt;sup&gt;text&lt;/sup&gt; uses &lt;code&gt;&amp;lt;sup&amp;gt;&lt;/code&gt; and subscript &lt;sub&gt;text&lt;/sub&gt; uses &lt;code&gt;&amp;lt;sub&amp;gt;&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Most of these elements are styled by browsers with few modifications on our part.&lt;/p&gt;

&lt;h2&gt;Heading&lt;/h2&gt;

&lt;p&gt;Vivamus sagittis lacus vel augue rutrum faucibus dolor auctor. Duis mollis, est non commodo luctus, nisi erat porttitor ligula, eget lacinia odio sem nec elit. Morbi leo risus, porta ac consectetur ac, vestibulum at eros.&lt;/p&gt;

&lt;h3&gt;Code&lt;/h3&gt;

&lt;p&gt;Cum sociis natoque penatibus et magnis dis &lt;code&gt;code element&lt;/code&gt; montes, nascetur ridiculus mus.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-js&quot; data-lang=&quot;js&quot;&gt;&lt;span class=&quot;c1&quot;&gt;// Example can be run directly in your JavaScript console&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// Create a function that takes two arguments and returns the sum of those arguments&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;adder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;Function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;a&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;b&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;return a + b&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// Call the function&lt;/span&gt;
&lt;span class=&quot;nx&quot;&gt;adder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// &amp;gt; 8&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Aenean lacinia bibendum nulla sed consectetur. Etiam porta sem malesuada magna mollis euismod. Fusce dapibus, tellus ac cursus commodo, tortor mauris condimentum nibh, ut fermentum massa.&lt;/p&gt;

&lt;h3&gt;Lists&lt;/h3&gt;

&lt;p&gt;Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Aenean lacinia bibendum nulla sed consectetur. Etiam porta sem malesuada magna mollis euismod. Fusce dapibus, tellus ac cursus commodo, tortor mauris condimentum nibh, ut fermentum massa justo sit amet risus.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Praesent commodo cursus magna, vel scelerisque nisl consectetur et.&lt;/li&gt;
&lt;li&gt;Donec id elit non mi porta gravida at eget metus.&lt;/li&gt;
&lt;li&gt;Nulla vitae elit libero, a pharetra augue.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Donec ullamcorper nulla non metus auctor fringilla. Nulla vitae elit libero, a pharetra augue.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Vestibulum id ligula porta felis euismod semper.&lt;/li&gt;
&lt;li&gt;Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.&lt;/li&gt;
&lt;li&gt;Maecenas sed diam eget risus varius blandit sit amet non magna.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Cras mattis consectetur purus sit amet fermentum. Sed posuere consectetur est at lobortis.&lt;/p&gt;

&lt;dl&gt;
  &lt;dt&gt;HyperText Markup Language (HTML)&lt;/dt&gt;
  &lt;dd&gt;The language used to describe and define the content of a Web page&lt;/dd&gt;

  &lt;dt&gt;Cascading Style Sheets (CSS)&lt;/dt&gt;
  &lt;dd&gt;Used to describe the appearance of Web content&lt;/dd&gt;

  &lt;dt&gt;JavaScript (JS)&lt;/dt&gt;
  &lt;dd&gt;The programming language used to build advanced Web sites and applications&lt;/dd&gt;
&lt;/dl&gt;

&lt;p&gt;Integer posuere erat a ante venenatis dapibus posuere velit aliquet. Morbi leo risus, porta ac consectetur ac, vestibulum at eros. Nullam quis risus eget urna mollis ornare vel eu leo.&lt;/p&gt;

&lt;h3&gt;Tables&lt;/h3&gt;

&lt;p&gt;Aenean lacinia bibendum nulla sed consectetur. Lorem ipsum dolor sit amet, consectetur adipiscing elit.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Name&lt;/th&gt;
      &lt;th&gt;Upvotes&lt;/th&gt;
      &lt;th&gt;Downvotes&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tfoot&gt;
    &lt;tr&gt;
      &lt;td&gt;Totals&lt;/td&gt;
      &lt;td&gt;21&lt;/td&gt;
      &lt;td&gt;23&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tfoot&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Alice&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Bob&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Charlie&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Nullam id dolor id nibh ultricies vehicula ut id elit. Sed posuere consectetur est at lobortis. Nullam quis risus eget urna mollis ornare vel eu leo.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;Want to see something else added? &lt;a href=&quot;https://github.com/poole/poole/issues/new&quot;&gt;Open an issue.&lt;/a&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>What's Jekyll?</title>
   <link href="pi19404.github.io/2013/12/31/whats-jekyll/"/>
   <updated>2013-12-31T00:00:00+05:30</updated>
   <id>pi19404.github.io/2013/12/31/whats-jekyll</id>
   <content type="html">&lt;p&gt;&lt;a href=&quot;http://jekyllrb.com&quot;&gt;Jekyll&lt;/a&gt; is a static site generator, an open-source tool for creating simple yet powerful websites of all shapes and sizes. From &lt;a href=&quot;https://github.com/mojombo/jekyll/blob/master/README.markdown&quot;&gt;the project&amp;#39;s readme&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Jekyll is a simple, blog aware, static site generator. It takes a template directory [...] and spits out a complete, static website suitable for serving with Apache or your favorite web server. This is also the engine behind GitHub Pages, which you can use to host your project’s page or blog right here from GitHub.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;It&amp;#39;s an immensely useful tool and one we encourage you to use here with Hyde.&lt;/p&gt;

&lt;p&gt;Find out more by &lt;a href=&quot;https://github.com/mojombo/jekyll&quot;&gt;visiting the project on GitHub&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 

</feed>
